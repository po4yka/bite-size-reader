"""Refactored URL processor using modular components."""

# ruff: noqa: E501
# flake8: noqa

from __future__ import annotations

import asyncio
import json
import logging
from collections.abc import Awaitable, Callable, Coroutine, Mapping
from functools import lru_cache
from pathlib import Path
from typing import TYPE_CHECKING, Any, cast

from app.adapters.content.content_chunker import ContentChunker
from app.adapters.content.content_extractor import ContentExtractor
from app.adapters.content.llm_summarizer import LLMSummarizer
from app.adapters.external.firecrawl_parser import FirecrawlClient
from app.adapters.openrouter.openrouter_client import OpenRouterClient
from app.adapters.telegram.message_persistence import MessagePersistence
from app.config import AppConfig
from app.core.async_utils import raise_if_cancelled
from app.core.lang import LANG_RU, choose_language
from app.core.url_utils import normalize_url, url_hash_sha256
from app.db.database import Database
from app.db.user_interactions import async_safe_update_user_interaction

if TYPE_CHECKING:
    from app.adapters.external.response_formatter import ResponseFormatter

logger = logging.getLogger(__name__)


_PROMPT_DIR = Path(__file__).resolve().parents[1] / "prompts"


# Built-in fallbacks to avoid ever sending an under-specified prompt if files are missing
_DEFAULT_SYSTEM_PROMPT_RU = """
Ð’Ñ‹ â€” ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚-ÑÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð´Ð»Ñ Ð½Ð°ÑˆÐµÐ³Ð¾ Telegram-Ð±Ð¾Ñ‚Ð°. Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ð¹Ñ‚Ðµ Ð¢ÐžÐ›Ð¬ÐšÐž ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON-Ð¾Ð±ÑŠÐµÐºÑ‚ Ð¿Ð¾ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÐºÑ‚Ñƒ Ð½Ð¸Ð¶Ðµ. ÐÐ¸ÐºÐ°ÐºÐ¸Ñ… Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ð¹, Markdown, Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¾Ð² Ð¸Ð»Ð¸ ÐºÐ¾Ð´-Ð±Ð»Ð¾ÐºÐ¾Ð² Ð²Ð½Ðµ JSON. Ð’Ñ‹Ð²Ð¾Ð´ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð² UTF-8 Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒÑÑ Ð±ÐµÐ· Ð¾ÑˆÐ¸Ð±Ð¾Ðº.

ÐšÐ»ÑŽÑ‡Ð¸ Ð²ÐµÑ€Ñ…Ð½ÐµÐ³Ð¾ ÑƒÑ€Ð¾Ð²Ð½Ñ (Ñ€Ð¾Ð²Ð½Ð¾ ÑÑ‚Ð¸, Ð±ÐµÐ· Ð»Ð¸ÑˆÐ½Ð¸Ñ…):
- summary_250
- summary_1000
- tldr
- key_ideas
- topic_tags
- entities
- estimated_reading_time_min
- key_stats
- answered_questions
- readability
- seo_keywords
- metadata
- extractive_quotes
- highlights
- questions_answered
- categories
- topic_taxonomy
- hallucination_risk
- confidence
- forwarded_post_extras
- key_points_to_remember
- insights
- article_id
- query_expansion_keywords
- semantic_boosters
- semantic_chunks

Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ðº Ð¿Ð¾Ð»ÑÐ¼:
- summary_250: ÑÑ‚Ñ€Ð¾ÐºÐ°, Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 250 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð². ÐžÐ´Ð½Ð° Ñ‘Ð¼ÐºÐ°Ñ Ñ„Ñ€Ð°Ð·Ð° (Ð»Ð¸Ð´ Ð² Ð½Ð¾Ð²Ð¾ÑÑ‚Ð½Ð¾Ð¼ ÑÑ‚Ð¸Ð»Ðµ) Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½Ð½Ð¾Ð¹ Ð¼Ñ‹ÑÐ»ÑŒÑŽ; Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð¾Ñ‚Ð»Ð¸Ñ‡Ð°Ñ‚ÑŒÑÑ Ð¾Ñ‚ summary_1000 Ð¸ TL;DR.
- summary_1000: ÑÑ‚Ñ€Ð¾ÐºÐ°, Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 1000 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð². 3â€“5 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼, Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð¾Ð¹, Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¾Ð¼ Ð¸ Ð¸Ñ‚Ð¾Ð³Ð°Ð¼Ð¸; Ð¾Ð½Ð° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½ÐµÐµ TL;DR Ð¸ Ð½Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÑ‚ÑŒ ÐµÐ³Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð¸Ð»Ð¸ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð´Ð¾ÑÐ»Ð¾Ð²Ð½Ð¾.
- tldr: ÑÑ‚Ñ€Ð¾ÐºÐ° Ð±ÐµÐ· Ð¶Ñ‘ÑÑ‚ÐºÐ¾Ð³Ð¾ Ð»Ð¸Ð¼Ð¸Ñ‚Ð° Ð´Ð»Ð¸Ð½Ñ‹. 2â€“3 Ð°Ð±Ð·Ð°Ñ†Ð° (Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ñ‹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð°Ð¼Ð¸ ÑÑ‚Ñ€Ð¾Ðº) Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼, Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÑÐ¼Ð¸, Ñ„Ð°ÐºÑ‚Ð°Ð¼Ð¸/Ñ†Ð¸Ñ„Ñ€Ð°Ð¼Ð¸ Ð¸ Ð²Ñ‹Ð²Ð¾Ð´Ð°Ð¼Ð¸. TL;DR Ð¾Ð±ÑÐ·Ð°Ð½ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÑ‚ÑŒ summary_1000 Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð´ÐµÑ‚Ð°Ð»ÑÐ¼Ð¸/Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ð¼Ð¸ Ð¸ Ð½Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÑ‚ÑŒ ÐµÐ³Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ.
- key_ideas: Ð¼Ð°ÑÑÐ¸Ð² ÐºÑ€Ð°Ñ‚ÐºÐ¸Ñ… ÑÑ‚Ñ€Ð¾Ðº (3â€“10). Ð‘ÐµÐ· Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð².
- topic_tags: Ð¼Ð°ÑÑÐ¸Ð² ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ñ… ÑÑ‚Ñ€Ð¾Ðº (1â€“3 ÑÐ»Ð¾Ð²Ð°, 3â€“8 ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²). Ð’ Ð½Ð¸Ð¶Ð½ÐµÐ¼ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ðµ, Ð±ÐµÐ· Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸, Ð±ÐµÐ· Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð².
- entities: Ð¾Ð±ÑŠÐµÐºÑ‚ Ñ Ð¼Ð°ÑÑÐ¸Ð²Ð°Ð¼Ð¸ { people, organizations, locations }, Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ñ€Ð¾ÐºÐ¸, Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¸ Ð±ÐµÐ· Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð².
- estimated_reading_time_min: Ñ†ÐµÐ»Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾ â‰¥ 0 (Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ° Ð² Ð¼Ð¸Ð½ÑƒÑ‚Ð°Ñ…).
- key_stats: Ð¼Ð°ÑÑÐ¸Ð² Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ { label: string, value: number, unit: string|null, source_excerpt: string|null }. Ð§Ð¸ÑÐ»Ð¾Ð²Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² Ð¿Ð¾Ð»Ðµ value; unit Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ (null), ÐµÑÐ»Ð¸ Ð½ÐµÐ¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾.
- answered_questions: Ð¼Ð°ÑÑÐ¸Ð² ÑÑ‚Ñ€Ð¾Ðº Ñ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð». Ð‘ÐµÐ· Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð².
- readability: Ð¾Ð±ÑŠÐµÐºÑ‚ { method: string (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Â«Flesch-KincaidÂ»), score: number, level: string }.
- seo_keywords: Ð¼Ð°ÑÑÐ¸Ð² ÑÑ‚Ñ€Ð¾Ðº (5â€“15), Ð² Ð½Ð¸Ð¶Ð½ÐµÐ¼ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ðµ, Ð±ÐµÐ· Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð².
- metadata: Ð¾Ð±ÑŠÐµÐºÑ‚ Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ { title, canonical_url, domain, author, published_at, last_updated } (ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð¸Ð»Ð¸ null). Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐ¹Ñ‚Ðµ Ð¸Ð· ÑÑ‚Ð°Ñ‚ÑŒÐ¸, ÐµÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹.
- extractive_quotes: Ð¼Ð°ÑÑÐ¸Ð² Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² { text: string, source_span: string|null } â€” Ð´Ð¾ 5 Ð´Ð¾ÑÐ»Ð¾Ð²Ð½Ñ‹Ñ… Ñ†Ð¸Ñ‚Ð°Ñ‚.
- highlights: Ð¼Ð°ÑÑÐ¸Ð² Ð¸Ð· 5â€“10 ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ñ… Ð¿ÑƒÐ½ÐºÑ‚Ð¾Ð².
- questions_answered: Ð¼Ð°ÑÑÐ¸Ð² Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² { question: string, answer: string } Ñ Ð¿Ð°Ñ€Ð°Ð¼Ð¸ Â«Ð²Ð¾Ð¿Ñ€Ð¾Ñâ€“Ð¾Ñ‚Ð²ÐµÑ‚Â».
- categories: Ð¼Ð°ÑÑÐ¸Ð² ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ñ… Ð¼ÐµÑ‚Ð¾Ðº ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹.
- topic_taxonomy: Ð¼Ð°ÑÑÐ¸Ð² Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² { label: string, score: number, path: string|null } Ð´Ð»Ñ Ð¸ÐµÑ€Ð°Ñ€Ñ…Ð¸Ð¸ Ñ‚ÐµÐ¼.
- hallucination_risk: ÑÑ‚Ñ€Ð¾ÐºÐ° low|med|high, Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÑŽÑ‰Ð°Ñ Ñ€Ð¸ÑÐº Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹.
- confidence: Ñ‡Ð¸ÑÐ»Ð¾ Ð¾Ñ‚ 0 Ð´Ð¾ 1, Ð¾Ñ‚Ñ€Ð°Ð¶Ð°ÑŽÑ‰ÐµÐµ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ð¸.
- forwarded_post_extras: Ð¾Ð±ÑŠÐµÐºÑ‚ Ð¸Ð»Ð¸ null Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Telegram { channel_id, channel_title, channel_username, message_id, post_datetime, hashtags, mentions }.
- key_points_to_remember: Ð¼Ð°ÑÑÐ¸Ð² ÑÑ‚Ñ€Ð¾Ðº Ñ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ð¼Ð¸ Ð²Ñ‹Ð²Ð¾Ð´Ð°Ð¼Ð¸.
- article_id: ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð²Ñ‹Ð¹ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÐºÐ°Ð½Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ URL Ð¸Ð»Ð¸ Ñ…ÐµÑˆ, ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾).
- query_expansion_keywords: Ð¼Ð°ÑÑÐ¸Ð² Ð¸Ð· 20â€“30 ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ñ… Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ñ… Ñ„Ñ€Ð°Ð·, Ð¾Ñ‚Ñ€Ð°Ð¶Ð°ÑŽÑ‰Ð¸Ñ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ (ÑÐ¸Ð½Ð¾Ð½Ð¸Ð¼Ñ‹, Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ¸, ÑƒÐ·ÐºÐ¸Ðµ/ÑˆÐ¸Ñ€Ð¾ÐºÐ¸Ðµ Ñ‚Ñ€Ð°ÐºÑ‚Ð¾Ð²ÐºÐ¸). Ð‘ÐµÐ· Ð´ÑƒÐ±Ð»ÐµÐ¹.
- semantic_boosters: Ð¼Ð°ÑÑÐ¸Ð² Ð¸Ð· 8â€“15 Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ñ… Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ñ… Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹, Ñ„Ð¸ÐºÑÐ¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ… ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ²ÑÐ·Ð¸/Ñ‚Ñ€ÐµÐ¹Ð´-Ð¾Ñ„Ñ„Ñ‹/ÑƒÑ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸Ñ, Ð¿Ñ€Ð¸Ð³Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð².
- semantic_chunks: Ð¼Ð°ÑÑÐ¸Ð² Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð´Ð»Ñ 100â€“200 ÑÐ»Ð¾Ð²Ð½Ñ‹Ñ…, Ð½ÐµÐ¿ÐµÑ€ÐµÑÐµÐºÐ°ÑŽÑ‰Ð¸Ñ…ÑÑ Ñ‡Ð°Ð½ÐºÐ¾Ð² Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ { text, local_summary, local_keywords, section|null, language|null, topics: [ÑÑ‚Ñ€Ð¾ÐºÐ¸], article_id }. `local_summary` â€” 1â€“2 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ; `local_keywords` â€” 3â€“8 Ñ„Ñ€Ð°Ð·.
- insights: Ð¾Ð±ÑŠÐµÐºÑ‚ ÑÐ¾ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸:
  * topic_overview: ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ ÑˆÐ¸Ñ€Ð¾ÐºÐ¸Ð¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼.
  * new_facts: Ð¼Ð°ÑÑÐ¸Ð² Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² { fact: string, why_it_matters: string|null, source_hint: string|null, confidence: number|string|null } Ñ 4â€“6 Ð½Ð¾Ð²Ñ‹Ð¼Ð¸ Ñ„Ð°ÐºÑ‚Ð°Ð¼Ð¸.
  * open_questions: Ð¼Ð°ÑÑÐ¸Ð² ÑÑ‚Ñ€Ð¾Ðº (3â€“6 Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¹).
  * suggested_sources: Ð¼Ð°ÑÑÐ¸Ð² ÑÑ‚Ñ€Ð¾Ðº Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°Ð¼Ð¸ Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ.
  * expansion_topics: Ð¼Ð°ÑÑÐ¸Ð² ÑÑ‚Ñ€Ð¾Ðº Ñ ÑÐ¼ÐµÐ¶Ð½Ñ‹Ð¼Ð¸ Ñ‚ÐµÐ¼Ð°Ð¼Ð¸.
  * next_exploration: Ð¼Ð°ÑÑÐ¸Ð² ÑÑ‚Ñ€Ð¾Ðº Ñ Ð¸Ð´ÐµÑÐ¼Ð¸ Ð´Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐ¸Ñ… ÑˆÐ°Ð³Ð¾Ð²/Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·.
  * caution: ÑÑ‚Ñ€Ð¾ÐºÐ° Ð¸Ð»Ð¸ null Ñ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸ÑÐ¼Ð¸/Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸ÑÐ¼Ð¸.

ÐŸÑ€Ð°Ð²Ð¸Ð»Ð°:
- ÐŸÐ¸ÑˆÐ¸Ñ‚Ðµ Ð½Ð° ÑÐ·Ñ‹ÐºÐµ, Ð·Ð°Ð¿Ñ€Ð¾ÑˆÐµÐ½Ð½Ð¾Ð¼ Ð² Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¾Ð¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¸.
- Ð”ÐµÑ€Ð¶Ð¸Ñ‚Ðµ `summary_250`, `summary_1000` Ð¸ `tldr` Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾ Ð³Ð»ÑƒÐ±Ð¸Ð½Ðµ: `summary_250` â€” Ð¾Ð´Ð½Ð° Ñ„Ñ€Ð°Ð·Ð°-Ñ…ÑƒÐº, `summary_1000` â€” ÑÐ¶Ð°Ñ‚Ð¾Ðµ Ð¿Ð¾Ð²ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð· Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹, `tldr` â€” ÑÐ°Ð¼Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¹ Ð¼Ð½Ð¾Ð³Ð¾Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ð½Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð·; Ð¸Ð·Ð±ÐµÐ³Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÐµÐ½Ð¸Ñ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¸Ð¼Ð¸.
- Ð‘ÑƒÐ´ÑŒÑ‚Ðµ Ñ„Ð°ÐºÑ‚Ð¸Ñ‡Ð½Ñ‹; Ð½Ðµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹Ñ‚Ðµ Ð¸Ð¼ÐµÐ½Ð°, Ñ‡Ð¸ÑÐ»Ð°, Ñ†Ð¸Ñ‚Ð°Ñ‚Ñ‹, URL Ð¸Ð»Ð¸ Ð´Ð°Ñ‚Ñ‹. Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½ÐµÑ‚, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ð¼Ð°ÑÑÐ¸Ð²Ñ‹ Ð¸Ð»Ð¸ null Ñ‚Ð°Ð¼, Ð³Ð´Ðµ ÑÑ‚Ð¾ Ð´Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼Ð¾; Ð½Ðµ Ð²ÑÑ‚Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ¸ Ð²Ñ€Ð¾Ð´Ðµ Â«N/AÂ».
- Ð¡Ñ‚Ñ€Ð¾Ð³Ð¾ ÑÐ¾Ð±Ð»ÑŽÐ´Ð°Ð¹Ñ‚Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹ (summary_250 Ð¸ summary_1000). ÐžÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ Ð´ÐµÐ»Ð°Ð¹Ñ‚Ðµ Ð»Ð°ÐºÐ¾Ð½Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸; Ð¸Ð·Ð±ÐµÐ³Ð°Ð¹Ñ‚Ðµ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¹ Ð±ÐµÐ· Ð½ÑƒÐ¶Ð´Ñ‹.
- Ð£Ð´Ð°Ð»ÑÐ¹Ñ‚Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ Ð² Ð¼Ð°ÑÑÐ¸Ð²Ð°Ñ… Ð¸ Ð² ÑÐ¿Ð¸ÑÐºÐ°Ñ… ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹. topic_tags â€” Ð² Ð½Ð¸Ð¶Ð½ÐµÐ¼ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ðµ, Ð±ÐµÐ· Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸; entities â€” Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ñ‹ Ð¸ Ð½Ðµ Ð¿ÑƒÑÑ‚Ñ‹Ðµ.
- Extractive quotes Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð´Ð¾ÑÐ»Ð¾Ð²Ð½Ñ‹Ð¼Ð¸ Ñ†Ð¸Ñ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°; Ð¿Ñ€Ð¸ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¼Ð°ÑÑÐ¸Ð². `source_excerpt` Ð² key_stats â€” Ð´Ð¾ÑÐ»Ð¾Ð²Ð½Ð¾Ðµ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸Ðµ, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ.
- ÐÐµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Ð»Ð¸ÑˆÐ½Ð¸Ñ… ÐºÐ»ÑŽÑ‡ÐµÐ¹, ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ² Ð¸Ð»Ð¸ Markdown; Ð½Ðµ Ð¼ÐµÐ½ÑÐ¹Ñ‚Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÐºÐ»ÑŽÑ‡ÐµÐ¹.
- ÐÐ• Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Markdown, ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ Ð¸Ð»Ð¸ Ð»ÑŽÐ±Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð¼Ð»ÑÑŽÑ‰Ð¸Ð¹ Ñ‚ÐµÐºÑÑ‚.
- ÐŸÐµÑ€ÐµÐ´ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¾Ð¹ ÑÐ´ÐµÐ»Ð°Ð¹Ñ‚Ðµ ÑÐ°Ð¼Ð¾Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ: summary_250, summary_1000 Ð¸ tldr Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð½ÐµÐ¿ÑƒÑÑ‚Ñ‹Ð¼Ð¸, Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð°Ñ‚ÑŒÑÑ Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ°Ð¼ Ð¸ ÑÐ¾Ð±Ð»ÑŽÐ´Ð°Ñ‚ÑŒ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹ Ð´Ð»Ð¸Ð½Ñ‹. Ð•ÑÐ»Ð¸ ÐºÐ°ÐºÐ¾Ð¹-Ñ‚Ð¾ Ð¸Ð· Ð½Ð¸Ñ… Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¸Ð»Ð¸ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÐµÑ‚ Ð´Ñ€ÑƒÐ³Ð¾Ð¹, ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON Ð·Ð°Ð½Ð¾Ð²Ð¾, Ð° Ð½Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ð¿Ð¾Ð»Ñ.
""".strip()

_DEFAULT_SYSTEM_PROMPT_EN = """
You are a structured summarization agent for our Telegram bot. Return ONLY a valid JSON object that strictly matches the contract below. No prose, headers, code fences, Markdown, or explanations outside the JSON. Output must be valid UTF-8 and parseable.

Top-level keys (exactly these, no extras):
- summary_250
- summary_1000
- tldr
- key_ideas
- topic_tags
- entities
- estimated_reading_time_min
- key_stats
- answered_questions
- readability
- seo_keywords
- metadata
- extractive_quotes
- highlights
- questions_answered
- categories
- topic_taxonomy
- hallucination_risk
- confidence
- forwarded_post_extras
- key_points_to_remember
- insights
- article_id
- query_expansion_keywords
- semantic_boosters
- semantic_chunks

Field requirements:
- summary_250: string, max 250 characters. One high-signal sentence (news lead style) that ends cleanly; must differ from summary_1000 and TL;DR wording.
- summary_1000: string, max 1000 characters. 3â€“5 sentences covering context, problem, approach, and outcomes; denser/shorter than TL;DR; do not reuse TL;DR sentences or phrasing verbatim.
- tldr: string, no hard length limit. 2â€“3 paragraphs separated by line breaks with context, actions, evidence, numbers, and implications. Expand beyond summary_1000 with additional details/examples; avoid repeating summary_1000 sentences or wording.
- key_ideas: array of concise strings (3â€“10 items). No duplicates.
- topic_tags: array of short strings (1â€“3 words each, 3â€“8 items). Lowercase, no punctuation, no duplicates.
- entities: object with arrays { people, organizations, locations }, strings only, deduplicated and normalized.
- estimated_reading_time_min: integer >= 0 (approx minutes to read the full source).
- key_stats: array of objects with keys { label: string, value: number, unit: string|null, source_excerpt: string|null }. Use numbers for value; omit units if not applicable.
- answered_questions: array of strings capturing questions the content answers. No duplicates.
- readability: object { method: string (e.g., "Flesch-Kincaid"), score: number, level: string }.
- seo_keywords: array of strings (5â€“15 items), lowercase, deduplicated.
- metadata: object with keys { title, canonical_url, domain, author, published_at, last_updated } (strings or null). Fill from article when available.
- extractive_quotes: array of objects { text: string, source_span: string|null } representing up to 5 verbatim pull quotes.
- highlights: array of 5â€“10 short bullet strings.
- questions_answered: array of objects { question: string, answer: string } describing Q&A pairs surfaced in the content.
- categories: array of short classification strings.
- topic_taxonomy: array of objects { label: string, score: number, path: string|null } for hierarchical categories.
- hallucination_risk: string enum low|med|high summarizing factual confidence.
- confidence: number between 0 and 1 expressing overall certainty.
- forwarded_post_extras: object|null capturing Telegram metadata { channel_id, channel_title, channel_username, message_id, post_datetime, hashtags, mentions }.
- key_points_to_remember: array of strings listing enduring takeaways.
- article_id: string identifier (use canonical URL or hash when available).
- query_expansion_keywords: array of 20â€“30 short English phrases a user might search (synonyms, alternative phrasings, specific/general intents). Deduplicated, lower risk of overlap.
- semantic_boosters: array of 8â€“15 standalone English sentences capturing key relationships/trade-offs/claims suitable for embeddings.
- semantic_chunks: array of objects, each representing a 100â€“200 word, non-overlapping chunk with keys { text, local_summary, local_keywords, section|null, language|null, topics: [strings], article_id }. `local_summary` must be 1â€“2 sentences; `local_keywords` 3â€“8 phrases.
- insights: object with keys:
  * topic_overview: string summarizing broader context.
  * new_facts: array of objects { fact: string, why_it_matters: string|null, source_hint: string|null, confidence: number|string|null } containing 4â€“6 beyond-text insights.
  * open_questions: array of strings (3â€“6 items).
  * suggested_sources: array of strings referencing follow-up reading.
  * expansion_topics: array of strings with adjacent themes to explore.
  * next_exploration: array of strings outlining experiments, hypotheses, or next steps.
  * caution: string|null flagging caveats or uncertainty.

Rules:
- Output all strings in the language requested in the user message.
- Keep `summary_250`, `summary_1000`, and `tldr` meaningfully distinct: `summary_250` is a single-sentence hook, `summary_1000` is a compact multi-sentence overview, and `tldr` is the richest multi-paragraph narrative; avoid sentence reuse across them.
- Be factual; do not invent numbers, names, quotes, URLs, or dates. If unknown or not present, use empty arrays or null where allowed by the contract; otherwise omit the claim. Do not output placeholders like "N/A" or "unknown".
- Respect required length caps strictly (e.g., summary_250 and summary_1000). Keep other fields succinct even without hard limits; prefer short clauses over run-ons.
- Deduplicate across arrays and within entity lists. Keep topic_tags lowercase, no punctuation; entities normalized and non-empty.
- Extractive quotes must be verbatim spans from the content; leave empty array if none. `source_excerpt` in key_stats should be verbatim evidence when available.
- Do not add extra keys, comments, or Markdown; do not change key order or naming.
- Do NOT include Markdown, comments, or any wrapper text.
- Before responding, self-check that summary_250, summary_1000, and tldr are all non-empty, mutually distinct in wording, and respect the stated length caps. If any are empty or duplicates, regenerate the JSON instead of returning blanks.
""".strip()


@lru_cache(maxsize=4)
def _get_system_prompt(lang: str) -> str:
    """Load and cache the system prompt for the given language."""
    fname = "summary_system_ru.txt" if lang == "ru" else "summary_system_en.txt"
    path = _PROMPT_DIR / fname
    try:
        prompt_text = path.read_text(encoding="utf-8").strip()
        # Guard against truncated/empty prompt files
        if "summary_250" not in prompt_text or "summary_1000" not in prompt_text:
            logger.warning(
                "system_prompt_missing_contract_keys",
                extra={"prompt_file": str(path), "lang": lang},
            )
            raise ValueError("prompt_missing_keys")
        return prompt_text
    except Exception as exc:
        logger.warning(
            "system_prompt_load_failed",
            extra={"prompt_file": str(path), "lang": lang, "error": str(exc)},
        )
        return _DEFAULT_SYSTEM_PROMPT_RU if lang == "ru" else _DEFAULT_SYSTEM_PROMPT_EN


class URLProcessor:
    """Refactored URL processor using modular components."""

    def __init__(
        self,
        cfg: AppConfig,
        db: Database,
        firecrawl: FirecrawlClient,
        openrouter: OpenRouterClient,
        response_formatter: ResponseFormatter,
        audit_func: Callable[[str, str, dict], None],
        sem: Callable[[], Any],
    ) -> None:
        self.cfg = cfg
        self.db = db
        self.response_formatter = response_formatter
        self._audit = audit_func

        # Initialize modular components
        self.content_extractor = ContentExtractor(
            cfg=cfg,
            db=db,
            firecrawl=firecrawl,
            response_formatter=response_formatter,
            audit_func=audit_func,
            sem=sem,
        )

        self.content_chunker = ContentChunker(
            cfg=cfg,
            openrouter=openrouter,
            response_formatter=response_formatter,
            audit_func=audit_func,
            sem=sem,
        )

        self.llm_summarizer = LLMSummarizer(
            cfg=cfg,
            db=db,
            openrouter=openrouter,
            response_formatter=response_formatter,
            audit_func=audit_func,
            sem=sem,
        )

        self.message_persistence = MessagePersistence(db=db)

    def _schedule_persistence_task(
        self, coro: Coroutine[Any, Any, Any], correlation_id: str | None, label: str
    ) -> asyncio.Task[Any] | None:
        """Run a persistence task without blocking the main flow."""
        try:
            task: asyncio.Task[Any] = asyncio.create_task(coro)
        except RuntimeError as exc:
            logger.error(
                "persistence_task_schedule_failed",
                extra={"cid": correlation_id, "label": label, "error": str(exc)},
            )
            return None

        def _log_task_error(t: asyncio.Task) -> None:
            if t.cancelled():
                return
            exc = t.exception()
            if exc:
                logger.error(
                    "persistence_task_failed",
                    extra={"cid": correlation_id, "label": label, "error": str(exc)},
                )

        task.add_done_callback(_log_task_error)
        return task

    async def _await_persistence_task(self, task: asyncio.Task | None) -> None:
        """Await a scheduled persistence task when required (silent flows)."""
        if task is None:
            return
        try:
            await task
        except Exception as exc:  # noqa: BLE001
            raise_if_cancelled(exc)
            logger.error("persistence_task_failed", extra={"error": str(exc)})

    async def handle_url_flow(
        self,
        message: Any,
        url_text: str,
        *,
        correlation_id: str | None = None,
        interaction_id: int | None = None,
        silent: bool = False,
    ) -> None:
        """Handle complete URL processing flow from extraction to summarization.

        Args:
            silent: If True, suppress all Telegram responses and only persist to database
        """
        if await self._maybe_reply_with_cached_summary(
            message,
            url_text,
            correlation_id=correlation_id,
            interaction_id=interaction_id,
            silent=silent,
        ):
            return

        try:
            norm = normalize_url(url_text)
            dedupe_hash = url_hash_sha256(norm)
            # Extract and process content
            (
                req_id,
                content_text,
                content_source,
                detected,
            ) = await self.content_extractor.extract_and_process_content(
                message, url_text, correlation_id, interaction_id, silent
            )

            # Choose language and load system prompt
            chosen_lang = choose_language(self.cfg.runtime.preferred_lang, detected)
            needs_ru_translation = not silent and detected != LANG_RU and chosen_lang != LANG_RU
            system_prompt = await self._load_system_prompt(chosen_lang)

            logger.debug(
                "language_choice",
                extra={"detected": detected, "chosen": chosen_lang, "cid": correlation_id},
            )

            # Notify: language detected with content preview (skip if silent)
            if not silent:
                content_preview = (
                    content_text[:150] + "..." if len(content_text) > 150 else content_text
                )
                await self.response_formatter.send_language_detection_notification(
                    message, detected, content_preview, url=url_text, silent=silent
                )

            # Check if content should be chunked
            should_chunk, max_chars, chunks = self.content_chunker.should_chunk_content(
                content_text, chosen_lang
            )

            if should_chunk and self.cfg.openrouter.long_context_model:
                logger.info(
                    "chunking_bypassed_long_context",
                    extra={
                        "cid": correlation_id,
                        "long_context_model": self.cfg.openrouter.long_context_model,
                        "content_length": len(content_text),
                    },
                )
                should_chunk = False
                chunks = None

            # Inform the user how the content will be handled (skip if silent)
            await self.response_formatter.send_content_analysis_notification(
                message,
                len(content_text),
                max_chars,
                should_chunk,
                chunks,
                self.cfg.openrouter.structured_output_mode,
                silent=silent,
            )

            logger.info(
                "content_handling",
                extra={
                    "cid": correlation_id,
                    "length": len(content_text),
                    "enable_chunking": should_chunk,
                    "threshold": max_chars,
                    "chunks": len(chunks or []) if should_chunk else 1,
                    "structured_output_enabled": self.cfg.openrouter.enable_structured_outputs,
                    "structured_output_mode": self.cfg.openrouter.structured_output_mode,
                },
            )

            # Process content based on chunking decision
            if should_chunk and chunks and len(chunks) > 1:
                # Process chunks and aggregate
                shaped = await self.content_chunker.process_chunks(
                    chunks, system_prompt, chosen_lang, req_id, correlation_id
                )

                if shaped:
                    shaped = self.llm_summarizer._enrich_with_rag_fields(
                        shaped,
                        content_text=content_text,
                        chosen_lang=chosen_lang,
                        req_id=req_id,
                    )
                    # Create stub LLM result for consistency
                    llm = self._create_chunk_llm_stub()

                    # Persist and respond (skip Telegram responses if silent)
                    await self._persist_and_respond_chunked(
                        message,
                        req_id,
                        chosen_lang,
                        shaped,
                        llm,
                        len(chunks),
                        correlation_id,
                        interaction_id,
                        needs_ru_translation,
                        silent=silent,
                    )
                    await self._maybe_send_russian_translation(
                        message,
                        shaped,
                        req_id,
                        correlation_id,
                        needs_ru_translation,
                    )
                    # Generate insights even in silent mode (for batch processing)
                    await self._handle_additional_insights(
                        message,
                        content_text,
                        chosen_lang,
                        req_id,
                        correlation_id,
                        summary=shaped,
                        silent=silent,
                    )
                    return
                else:
                    # Fallback to single-pass if chunking failed
                    logger.warning(
                        "chunking_failed_fallback_to_single", extra={"cid": correlation_id}
                    )

            # Single-pass summarization
            shaped = await self.llm_summarizer.summarize_content(
                message,
                content_text,
                chosen_lang,
                system_prompt,
                req_id,
                max_chars,
                correlation_id,
                interaction_id,
                url_hash=dedupe_hash,
                url=url_text,
                silent=silent,
                defer_persistence=True,
            )

            if shaped:
                llm_result = self.llm_summarizer.last_llm_result

                # Skip Telegram responses if silent
                if not silent:
                    await self.response_formatter.send_structured_summary_response(
                        message,
                        shaped,
                        llm_result,
                    )
                    logger.info(
                        "reply_json_sent", extra={"cid": correlation_id, "request_id": req_id}
                    )

                    await self._maybe_send_russian_translation(
                        message,
                        shaped,
                        req_id,
                        correlation_id,
                        needs_ru_translation,
                    )

                    # Notify user that we will attempt to generate extra research insights
                    try:
                        await self.response_formatter.safe_reply(
                            message,
                            "ðŸ§  Generating additional research insightsâ€¦",
                        )
                    except Exception as exc:
                        raise_if_cancelled(exc)
                        pass

                    await self._handle_additional_insights(
                        message,
                        content_text,
                        chosen_lang,
                        req_id,
                        correlation_id,
                        summary=shaped,
                    )

                    # Generate a standalone custom article based on extracted topics/tags
                    try:
                        topics = shaped.get("key_ideas") or []
                        tags = shaped.get("topic_tags") or []
                        if (topics or tags) and isinstance(topics, list) and isinstance(tags, list):
                            await self.response_formatter.safe_reply(
                                message,
                                "ðŸ“ Crafting a standalone article from topics & tagsâ€¦",
                            )
                            article = await self.llm_summarizer.generate_custom_article(
                                message,
                                chosen_lang=chosen_lang,
                                req_id=req_id,
                                topics=[str(x) for x in topics if str(x).strip()],
                                tags=[str(x) for x in tags if str(x).strip()],
                                correlation_id=correlation_id,
                            )
                            if article:
                                await self.response_formatter.send_custom_article(message, article)
                    except Exception as exc:  # noqa: BLE001
                        raise_if_cancelled(exc)
                        logger.error(
                            "custom_article_flow_error",
                            extra={"cid": correlation_id, "error": str(exc)},
                        )
                else:
                    # Silent mode: persist and generate insights without responses
                    new_version = await self.db.async_upsert_summary(
                        request_id=req_id,
                        lang=chosen_lang,
                        json_payload=shaped,
                        is_read=False,
                    )
                    await self.db.async_update_request_status(req_id, "ok")
                    self._audit(
                        "INFO", "summary_upserted", {"request_id": req_id, "version": new_version}
                    )

                    # Generate insights even in silent mode (for batch processing)
                    await self._handle_additional_insights(
                        message,
                        content_text,
                        chosen_lang,
                        req_id,
                        correlation_id,
                        summary=shaped,
                        silent=True,
                    )
                    logger.info(
                        "silent_summary_persisted",
                        extra={"cid": correlation_id, "request_id": req_id},
                    )

        except ValueError as e:
            # Handle known errors (like Firecrawl failures)
            logger.error("url_flow_error", extra={"error": str(e), "cid": correlation_id})
        except Exception as e:
            raise_if_cancelled(e)
            # Handle unexpected errors
            logger.exception(
                "url_flow_unexpected_error", extra={"error": str(e), "cid": correlation_id}
            )

    def _create_chunk_llm_stub(self) -> Any:
        """Create a stub LLM result for chunked processing."""
        return type(
            "LLMStub",
            (),
            {
                "status": "ok",
                "latency_ms": None,
                "model": self.cfg.openrouter.model,
                "cost_usd": None,
                "tokens_prompt": None,
                "tokens_completion": None,
                "structured_output_used": True,
                "structured_output_mode": self.cfg.openrouter.structured_output_mode,
            },
        )()

    async def _persist_and_respond_chunked(
        self,
        message: Any,
        req_id: int,
        chosen_lang: str,
        shaped: dict[str, Any],
        llm: Any,
        chunk_count: int,
        correlation_id: str | None,
        interaction_id: int | None,
        needs_ru_translation: bool,
        silent: bool = False,
    ) -> None:
        """Persist chunked results and send response."""
        try:

            async def _persist_chunk() -> None:
                new_version = await self.db.async_upsert_summary(
                    request_id=req_id,
                    lang=chosen_lang,
                    json_payload=shaped,
                    is_read=not silent,
                )
                await self.db.async_update_request_status(req_id, "ok")
                self._audit(
                    "INFO", "summary_upserted", {"request_id": req_id, "version": new_version}
                )

            persistence_task = self._schedule_persistence_task(
                _persist_chunk(), correlation_id, "chunk_summary_persist"
            )
            if silent:
                await self._await_persistence_task(persistence_task)
        except Exception as e:  # noqa: BLE001
            raise_if_cancelled(e)
            logger.error("persist_summary_error", extra={"error": str(e), "cid": correlation_id})

        if interaction_id:
            await async_safe_update_user_interaction(
                self.db,
                interaction_id=interaction_id,
                response_sent=True,
                response_type="summary",
                request_id=req_id,
                logger_=logger,
            )

        # Send structured results (skip if silent)
        if not silent:
            await self.response_formatter.send_structured_summary_response(
                message, shaped, llm, chunks=chunk_count
            )
            logger.info("reply_json_sent", extra={"cid": correlation_id, "request_id": req_id})

    async def _maybe_send_russian_translation(
        self,
        message: Any,
        summary: dict[str, Any],
        req_id: int,
        correlation_id: str | None,
        needs_translation: bool,
    ) -> None:
        """Generate and send an adapted Russian translation of the summary when required."""
        if not needs_translation:
            return

        try:
            translated = await self.llm_summarizer.translate_summary_to_ru(
                summary,
                req_id=req_id,
                correlation_id=correlation_id,
            )
            if translated:
                await self.response_formatter.send_russian_translation(
                    message, translated, correlation_id=correlation_id
                )
                return

            await self.response_formatter.safe_reply(
                message,
                f"âš ï¸ Unable to generate Russian translation right now. Error ID: {correlation_id or 'unknown'}.",
            )
        except Exception as exc:  # noqa: BLE001
            raise_if_cancelled(exc)
            logger.exception(
                "ru_translation_failed", extra={"cid": correlation_id, "error": str(exc)}
            )
            try:
                await self.response_formatter.safe_reply(
                    message,
                    f"âš ï¸ Russian translation failed. Error ID: {correlation_id or 'unknown'}.",
                )
            except Exception:
                pass

    async def _handle_additional_insights(
        self,
        message: Any,
        content_text: str,
        chosen_lang: str,
        req_id: int,
        correlation_id: str | None,
        *,
        summary: dict[str, Any] | None = None,
        silent: bool = False,
    ) -> None:
        """Generate and persist additional insights using the LLM."""
        logger.info(
            "insights_flow_started",
            extra={"cid": correlation_id, "content_len": len(content_text), "lang": chosen_lang},
        )

        try:
            insights = await self.llm_summarizer.generate_additional_insights(
                message,
                content_text=content_text,
                chosen_lang=chosen_lang,
                req_id=req_id,
                correlation_id=correlation_id,
                summary=summary,
            )

            if insights:
                logger.info(
                    "insights_generated_successfully",
                    extra={
                        "cid": correlation_id,
                        "facts_count": len(insights.get("new_facts", [])),
                        "has_overview": bool(insights.get("topic_overview")),
                    },
                )

                # Only send insights message if not in silent mode
                if not silent:
                    await self.response_formatter.send_additional_insights_message(
                        message, insights, correlation_id
                    )
                    logger.info("insights_message_sent", extra={"cid": correlation_id})
                else:
                    logger.info("insights_generated_silently", extra={"cid": correlation_id})

                try:
                    self.db.update_summary_insights(req_id, insights)
                    logger.debug(
                        "insights_persisted", extra={"cid": correlation_id, "request_id": req_id}
                    )
                except Exception as exc:  # noqa: BLE001
                    raise_if_cancelled(exc)
                    logger.error(
                        "persist_insights_error",
                        extra={"cid": correlation_id, "error": str(exc)},
                    )
            else:
                logger.warning(
                    "insights_generation_returned_empty",
                    extra={"cid": correlation_id, "reason": "LLM returned None or empty insights"},
                )

        except Exception as exc:  # noqa: BLE001
            raise_if_cancelled(exc)
            logger.exception(
                "insights_flow_error",
                extra={"cid": correlation_id, "error": str(exc)},
            )

    async def _load_system_prompt(self, lang: str) -> str:
        """Load system prompt file based on language."""
        return _get_system_prompt(lang)

    def _is_summary_complete(self, summary: dict[str, Any]) -> bool:
        """Check if a summary has the essential fields populated."""
        if not isinstance(summary, dict):
            return False

        # Essential fields that must be present and non-empty
        essential_fields = [
            ("summary_250", "summary_250"),
            ("summary_1000", "summary_1000"),
            ("tldr", "tldr"),
        ]

        # Check essential fields
        for field, lookup in essential_fields:
            value = summary.get(lookup)
            if lookup == "summary_1000" and not value:
                value = summary.get("tldr")
            if lookup == "tldr" and not value:
                value = summary.get("summary_1000")
            if not value or not str(value).strip():
                return False

        # At least one of these should have content
        content_fields = [
            "key_ideas",
            "topic_tags",
            "entities",
        ]

        has_content = False
        for field in content_fields:
            value = summary.get(field)
            if field == "entities":
                if isinstance(value, dict) and any(
                    isinstance(v, list) and len(v) > 0 for v in value.values()
                ):
                    has_content = True
                    break
            elif isinstance(value, list) and len(value) > 0:
                has_content = True
                break

        if not has_content:
            logger.debug(
                "cached_summary_missing_optional_content",
                extra={"missing_fields": content_fields},
            )

        return True

    def _get_missing_summary_fields(self, summary: dict[str, Any]) -> list[str]:
        """Get list of missing or empty essential fields."""
        if not isinstance(summary, dict):
            return ["invalid_summary_format"]

        missing = []
        essential_fields = [
            ("summary_250", "summary_250"),
            ("summary_1000", "summary_1000"),
            ("tldr", "tldr"),
        ]

        for field, lookup in essential_fields:
            value = summary.get(lookup)
            if lookup == "summary_1000" and not value:
                value = summary.get("tldr")
            if lookup == "tldr" and not value:
                value = summary.get("summary_1000")
            if not value or not str(value).strip():
                missing.append(field)

        return missing

    async def _maybe_reply_with_cached_summary(
        self,
        message: Any,
        url_text: str,
        *,
        correlation_id: str | None,
        interaction_id: int | None,
        silent: bool = False,
    ) -> bool:
        """Return True if an existing summary was reused."""
        try:
            norm = normalize_url(url_text)
        except Exception:
            return False

        dedupe = url_hash_sha256(norm)
        existing_req = await self.db.async_get_request_by_dedupe_hash(dedupe)
        if not isinstance(existing_req, Mapping):
            getter = getattr(self.db, "get_request_by_dedupe_hash", None)
            existing_req = getter(dedupe) if callable(getter) else None

        if isinstance(existing_req, Mapping):
            existing_req = dict(existing_req)

        if not existing_req:
            return False

        req_id = int(existing_req["id"])
        summary_row = await self.db.async_get_summary_by_request(req_id)
        if not isinstance(summary_row, Mapping):
            getter = getattr(self.db, "get_summary_by_request", None)
            summary_row = getter(req_id) if callable(getter) else None

        if isinstance(summary_row, Mapping):
            summary_row = dict(summary_row)

        if not summary_row:
            return False

        payload = summary_row.get("json_payload")
        if payload is None:
            logger.debug(
                "cached_summary_empty_payload",
                extra={"request_id": req_id, "cid": correlation_id},
            )
            return False

        if isinstance(payload, Mapping):
            shaped = dict(payload)
        elif isinstance(payload, str):
            try:
                shaped = json.loads(payload)
            except json.JSONDecodeError:
                logger.warning(
                    "cached_summary_decode_failed",
                    extra={"request_id": req_id, "cid": correlation_id},
                )
                return False
        else:
            logger.warning(
                "cached_summary_unsupported_payload",
                extra={"request_id": req_id, "cid": correlation_id, "type": type(payload).__name__},
            )
            return False

        # Validate that the summary is complete and has essential fields
        if not self._is_summary_complete(shaped):
            logger.debug(
                "cached_summary_incomplete",
                extra={
                    "request_id": req_id,
                    "cid": correlation_id,
                    "missing_fields": self._get_missing_summary_fields(shaped),
                },
            )
            return False

        if correlation_id:
            try:
                self.db.update_request_correlation_id(req_id, correlation_id)
            except Exception as exc:  # noqa: BLE001
                raise_if_cancelled(exc)
                logger.error("persist_cid_error", extra={"error": str(exc), "cid": correlation_id})

        # Skip Telegram responses if silent
        if not silent:
            await self.response_formatter.send_url_accepted_notification(
                message, norm, correlation_id or "", silent=silent
            )
            await self.response_formatter.send_cached_summary_notification(message, silent=silent)
            # Resolve model used previously for this request and pass a stub to avoid 'unknown'
            try:
                model_name = self.db.get_latest_llm_model_by_request_id(req_id)
            except Exception:
                model_name = None
            llm_stub = type("LLMStub", (), {"model": model_name})()
            await self.response_formatter.send_structured_summary_response(
                message, shaped, llm_stub
            )

            insights_raw = summary_row.get("insights_json")
            insights_payload: dict[str, Any] | None
            if isinstance(insights_raw, Mapping):
                insights_payload = dict(cast(Mapping[str, Any], insights_raw))
            elif isinstance(insights_raw, str) and insights_raw.strip():
                try:
                    decoded = json.loads(insights_raw)
                except json.JSONDecodeError:
                    logger.warning(
                        "cached_insights_decode_failed",
                        extra={"request_id": req_id, "cid": correlation_id},
                    )
                    decoded = None
                if isinstance(decoded, Mapping):
                    insights_payload = dict(cast(Mapping[str, Any], decoded))
                else:
                    insights_payload = None
            else:
                insights_payload = None
            if insights_payload:
                await self.response_formatter.send_additional_insights_message(
                    message, dict(insights_payload), correlation_id
                )

        await self.db.async_update_request_status(req_id, "ok")

        self._audit(
            "INFO",
            "summary_cache_hit",
            {
                "request_id": req_id,
                "url": norm,
                "cid": correlation_id,
            },
        )

        if interaction_id:
            await async_safe_update_user_interaction(
                self.db,
                interaction_id=interaction_id,
                response_sent=True,
                response_type="summary",
                request_id=req_id,
                logger_=logger,
            )

        logger.info(
            "summary_cache_reused",
            extra={"request_id": req_id, "cid": correlation_id, "normalized_url": norm},
        )
        return True
